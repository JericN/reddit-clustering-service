{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA) \n",
    "LDA is a tool for finding topics in a collection of documents. It assumes that each document is a mix of topics, and each topic is a mix of words. The goal is to uncover these topics from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package installation\n",
    "# %pip install --upgrade matplotlib\n",
    "# %pip install --upgrade numpy\n",
    "# %pip install --upgrade pandas\n",
    "# %pip install --upgrade seaborn\n",
    "# %pip install --upgrade scikit-learn\n",
    "# %pip install --upgrade scipy==1.12\n",
    "# %pip install --upgrade nltk\n",
    "# %pip install --upgrade wordcloud\n",
    "# %pip install --upgrade gensim\n",
    "# %pip install --upgrade pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "# Scientific computing\n",
    "import scipy\n",
    "# Regular expression operations\n",
    "import re\n",
    "# Common string operations\n",
    "import string \n",
    "\n",
    "# Interpret the results of the LDA model\n",
    "import pyLDAvis\n",
    "# Interactive data visualization\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# Data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Unsupervised topic modeling, document indexing.\n",
    "import gensim\n",
    "# Mapping of the words to integers\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import Phrases, phrases\n",
    "\n",
    "# Natural language processing\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('wordnet') \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# formatting\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# set pd column width\n",
    "pd.set_option('display.max_colwidth', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(dir: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(dir)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    # Get relevant data\n",
    "    data[\"title\"] = df[\"title\"].fillna(\"\")\n",
    "    data[\"body\"] = df[\"body\"].fillna(\"\")\n",
    "\n",
    "    # Combine title and body\n",
    "    data[\"text\"] = data[\"title\"] + \" \" + data[\"body\"]\n",
    "\n",
    "    # Remove links\n",
    "    data[\"processed\"] = data[\"text\"].map(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
    "\n",
    "    # Remove punctuation\n",
    "    data[\"processed\"] = data[\"processed\"].map(lambda x: x.translate(str.maketrans(\"\", \"\", string.punctuation)))\n",
    "\n",
    "    # Convert to lowercase\n",
    "    data[\"processed\"] = data[\"processed\"].map(lambda x: x.lower())\n",
    "\n",
    "    # Tokenize\n",
    "    data[\"processed\"] = data[\"processed\"].map(word_tokenize)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    data[\"processed\"] = data[\"processed\"].map(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "    # Remove words with less than 3 characters\n",
    "    data[\"processed\"] = data[\"processed\"].map(lambda x: [word for word in x if len(word) >= 3])\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    data[\"processed\"] = data[\"processed\"].map(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "    # Stemming\n",
    "    # stemmer = PorterStemmer()\n",
    "    # data['processed'] = data['processed'].map(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "    # Remove rows with less than 5 words\n",
    "    data = data[data[\"processed\"].map(len) >= 10]\n",
    "\n",
    "    # Remove unnecessary columns\n",
    "    data.drop([\"title\", \"body\"], axis=1, inplace=True)\n",
    "\n",
    "    # reset index\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngrams(texts, ngram_model):\n",
    "    return [ngram_model[doc] for doc in texts]\n",
    "\n",
    "\n",
    "def generate_ngrams(data, n=2, min_count=5, threshold=50):\n",
    "    dataset = data.copy()\n",
    "\n",
    "    for i in range(2, n+1):\n",
    "        ngram = Phrases(dataset, min_count=min_count, threshold=threshold)\n",
    "        ngram_model = phrases.Phraser(ngram)\n",
    "        dataset = make_ngrams(dataset, ngram_model)\n",
    "\n",
    "    return dataset\n",
    "    \n",
    "\n",
    "def print_ngrams(data, n=2):\n",
    "    ngrams_set = set()\n",
    "    for row in data:\n",
    "        for word in row:\n",
    "            if word.count(\"_\") >= n - 1:\n",
    "                ngrams_set.add(word)\n",
    "\n",
    "    print(\"length of ngrams set: \", len(ngrams_set))\n",
    "    print(ngrams_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dictionary and corpus needed for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(data: pd.DataFrame) -> tuple:\n",
    "    dataset = data.copy()\n",
    "\n",
    "    # Create a dictionary\n",
    "    id2word = corpora.Dictionary(dataset)\n",
    "    # Filter out words\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.6)\n",
    "\n",
    "    # Create a corpus\n",
    "    corpus = [id2word.doc2bow(text) for text in dataset]\n",
    "\n",
    "    return id2word, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to build the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda_model(id2word, corpus, alpha=\"symmetric\", beta=\"auto\", topics=10):\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        num_topics=topics,\n",
    "        random_state=100,\n",
    "        update_every=1,\n",
    "        chunksize=100,\n",
    "        passes=5,\n",
    "        alpha=alpha,\n",
    "        per_word_topics=True,\n",
    "        eta=beta,\n",
    "    )\n",
    "\n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coherence_score(data, id2word, corpus, alpha, beta, topics):\n",
    "    lda_model = train_lda_model(id2word, corpus, alpha, beta, topics)\n",
    "\n",
    "    coherence_model_lda = CoherenceModel(\n",
    "        model=lda_model,\n",
    "        texts=data,\n",
    "        dictionary=id2word,\n",
    "        coherence=\"c_v\",\n",
    "    )\n",
    "    coherence_score = coherence_model_lda.get_coherence()\n",
    "    return coherence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to iterate through hyperparameters\n",
    "def find_best_model(data, id2word, corpus, alpha_list, beta_list, topics_list):\n",
    "    # save param of highest coherence score\n",
    "    best_params = {\"score\": 0, \"alpha\": 0, \"beta\": 0, \"topics\": 0}\n",
    "\n",
    "    # loop through all hyperparameters\n",
    "    for topics in topics_list:\n",
    "        for alpha in alpha_list:\n",
    "            for beta in beta_list:\n",
    "                # calculate coherence score\n",
    "                coherence_score = calculate_coherence_score(data, id2word, corpus, alpha, beta, topics)\n",
    "                print(f\"topics={topics}, alpha={alpha}, beta={beta} -> Coherence Score: {coherence_score}\")\n",
    "\n",
    "                # update best params if new score is higher\n",
    "                if coherence_score > best_params[\"score\"]:\n",
    "                    best_params[\"score\"] = coherence_score\n",
    "                    best_params[\"alpha\"] = alpha\n",
    "                    best_params[\"beta\"] = beta\n",
    "                    best_params[\"topics\"] = topics\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START LDA MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "df = import_data(\"../data/data.csv\")\n",
    "\n",
    "# preprocess\n",
    "posts = preprocess_data(df)\n",
    "\n",
    "# generate n-grams\n",
    "posts[\"processed\"] = generate_ngrams(posts[\"processed\"], n=3, min_count=5, threshold=100)\n",
    "print(\"Data Shape:\", posts.shape)\n",
    "\n",
    "# create corpus\n",
    "id2word, corpus = create_corpus(posts[\"processed\"])\n",
    "print(\"Number of unique words:\", len(id2word))\n",
    "print(\"Number of documents:\", len(corpus))\n",
    "\n",
    "# hyperparameters\n",
    "no_of_topics = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "alpha_list = [\"symmetric\", 0.1, 0.5, 0.9]\n",
    "beta_list = [0.1, 0.5, 0.9]\n",
    "print(\"\\n\\nStarting hyperparameter tuning...\")\n",
    "\n",
    "# find best model\n",
    "best_params = find_best_model(posts[\"processed\"], id2word, corpus, alpha_list, beta_list, no_of_topics)\n",
    "print(best_params)\n",
    "\n",
    "\n",
    "# train lda model\n",
    "lda_model = train_lda_model(\n",
    "    id2word,\n",
    "    corpus,\n",
    "    best_params[\"alpha\"],\n",
    "    best_params[\"beta\"],\n",
    "    best_params[\"topics\"],\n",
    ")\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
