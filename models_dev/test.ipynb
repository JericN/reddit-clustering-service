{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA) \n",
    "LDA is a tool for finding topics in a collection of documents. It assumes that each document is a mix of topics, and each topic is a mix of words. The goal is to uncover these topics from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package installation\n",
    "# %pip install --upgrade matplotlib\n",
    "# %pip install --upgrade numpy\n",
    "# %pip install --upgrade pandas\n",
    "# %pip install --upgrade seaborn\n",
    "# %pip install --upgrade scikit-learn\n",
    "# %pip install --upgrade scipy==1.12\n",
    "# %pip install --upgrade nltk\n",
    "# %pip install --upgrade wordcloud\n",
    "# %pip install --upgrade gensim\n",
    "# %pip install --upgrade pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\skybl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\skybl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\skybl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "# Scientific computing\n",
    "import scipy\n",
    "# Regular expression operations\n",
    "import re\n",
    "# Common string operations\n",
    "import string \n",
    "\n",
    "# Interpret the results of the LDA model\n",
    "import pyLDAvis\n",
    "# Interactive data visualization\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# Data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Unsupervised topic modeling, document indexing.\n",
    "import gensim\n",
    "# Mapping of the words to integers\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Natural language processing\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('wordnet') \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# formatting\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# set pd column width\n",
    "pd.set_option('display.max_colwidth', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>permalink</th>\n",
       "      <th>author</th>\n",
       "      <th>tag</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>comments</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>movies</td>\n",
       "      <td>t3_1coi02o</td>\n",
       "      <td>1.715319e+09</td>\n",
       "      <td>/r/movies/commen...</td>\n",
       "      <td>t2_5fw4514m</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>Bottoms - some t...</td>\n",
       "      <td>It was a super f...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>movies</td>\n",
       "      <td>t3_1cohlro</td>\n",
       "      <td>1.715317e+09</td>\n",
       "      <td>/r/movies/commen...</td>\n",
       "      <td>t2_cq7rp7m1b</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>Presenting the c...</td>\n",
       "      <td>This cinematic u...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movies</td>\n",
       "      <td>t3_1coh6ks</td>\n",
       "      <td>1.715316e+09</td>\n",
       "      <td>/r/movies/commen...</td>\n",
       "      <td>t2_ruw91ssi8</td>\n",
       "      <td>Poster</td>\n",
       "      <td>New poster for ‘...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>movies</td>\n",
       "      <td>t3_1coh5hv</td>\n",
       "      <td>1.715316e+09</td>\n",
       "      <td>/r/movies/commen...</td>\n",
       "      <td>t2_i94zymonh</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>What is your fav...</td>\n",
       "      <td>Not sure if it's...</td>\n",
       "      <td>58</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>movies</td>\n",
       "      <td>t3_1cogzod</td>\n",
       "      <td>1.715315e+09</td>\n",
       "      <td>/r/movies/commen...</td>\n",
       "      <td>t2_kmbj6</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>I saw Godzilla M...</td>\n",
       "      <td>I know people ar...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5401</th>\n",
       "      <td>netflix</td>\n",
       "      <td>t3_1btkrw2</td>\n",
       "      <td>1.712018e+09</td>\n",
       "      <td>/r/netflix/comme...</td>\n",
       "      <td>t2_s8es9q0y8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nothing is good ...</td>\n",
       "      <td>I can't find a n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5402</th>\n",
       "      <td>netflix</td>\n",
       "      <td>t3_1btioz2</td>\n",
       "      <td>1.712012e+09</td>\n",
       "      <td>/r/netflix/comme...</td>\n",
       "      <td>t2_4wctnf1b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Homicide New Yor...</td>\n",
       "      <td>They never tell ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5403</th>\n",
       "      <td>netflix</td>\n",
       "      <td>t3_1bthtpp</td>\n",
       "      <td>1.712010e+09</td>\n",
       "      <td>/r/netflix/comme...</td>\n",
       "      <td>t2_4bm6zxbl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3 Body Problem T...</td>\n",
       "      <td>If they are only...</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5404</th>\n",
       "      <td>netflix</td>\n",
       "      <td>t3_1bthoxr</td>\n",
       "      <td>1.712010e+09</td>\n",
       "      <td>/r/netflix/comme...</td>\n",
       "      <td>t2_wlugo32oa</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Do you recommend...</td>\n",
       "      <td>For those who ha...</td>\n",
       "      <td>579</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5405</th>\n",
       "      <td>netflix</td>\n",
       "      <td>t3_1btgr7j</td>\n",
       "      <td>1.712008e+09</td>\n",
       "      <td>/r/netflix/comme...</td>\n",
       "      <td>t2_1crgx69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>White Collar Epi...</td>\n",
       "      <td>Apologies on mob...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5406 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subreddit          id     timestamp            permalink        author  \\\n",
       "0       movies  t3_1coi02o  1.715319e+09  /r/movies/commen...   t2_5fw4514m   \n",
       "1       movies  t3_1cohlro  1.715317e+09  /r/movies/commen...  t2_cq7rp7m1b   \n",
       "2       movies  t3_1coh6ks  1.715316e+09  /r/movies/commen...  t2_ruw91ssi8   \n",
       "3       movies  t3_1coh5hv  1.715316e+09  /r/movies/commen...  t2_i94zymonh   \n",
       "4       movies  t3_1cogzod  1.715315e+09  /r/movies/commen...      t2_kmbj6   \n",
       "...        ...         ...           ...                  ...           ...   \n",
       "5401   netflix  t3_1btkrw2  1.712018e+09  /r/netflix/comme...  t2_s8es9q0y8   \n",
       "5402   netflix  t3_1btioz2  1.712012e+09  /r/netflix/comme...   t2_4wctnf1b   \n",
       "5403   netflix  t3_1bthtpp  1.712010e+09  /r/netflix/comme...   t2_4bm6zxbl   \n",
       "5404   netflix  t3_1bthoxr  1.712010e+09  /r/netflix/comme...  t2_wlugo32oa   \n",
       "5405   netflix  t3_1btgr7j  1.712008e+09  /r/netflix/comme...    t2_1crgx69   \n",
       "\n",
       "             tag                title                 body  comments  score  \n",
       "0     Discussion  Bottoms - some t...  It was a super f...         0      1  \n",
       "1     Discussion  Presenting the c...  This cinematic u...         5      1  \n",
       "2         Poster  New poster for ‘...                  NaN         9      0  \n",
       "3     Discussion  What is your fav...  Not sure if it's...        58     17  \n",
       "4     Discussion  I saw Godzilla M...  I know people ar...        17      0  \n",
       "...          ...                  ...                  ...       ...    ...  \n",
       "5401         NaN  Nothing is good ...  I can't find a n...         0      0  \n",
       "5402         NaN  Homicide New Yor...  They never tell ...         2      1  \n",
       "5403         NaN  3 Body Problem T...  If they are only...        23      0  \n",
       "5404         NaN  Do you recommend...  For those who ha...       579    448  \n",
       "5405         NaN  White Collar Epi...  Apologies on mob...         1      4  \n",
       "\n",
       "[5406 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "df = pd.read_csv('../data/data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['official',\n",
       " 'discussion',\n",
       " 'kingdom',\n",
       " 'planet',\n",
       " 'ape',\n",
       " 'spoiler',\n",
       " 'poll',\n",
       " 'youve',\n",
       " 'seen',\n",
       " 'film',\n",
       " 'please',\n",
       " 'rate',\n",
       " 'poll',\n",
       " 'havent',\n",
       " 'seen',\n",
       " 'film',\n",
       " 'would',\n",
       " 'like',\n",
       " 'see',\n",
       " 'result',\n",
       " 'poll',\n",
       " 'click',\n",
       " 'ranking',\n",
       " 'click',\n",
       " 'see',\n",
       " 'ranking',\n",
       " '2024',\n",
       " 'film',\n",
       " 'click',\n",
       " 'see',\n",
       " 'ranking',\n",
       " 'every',\n",
       " 'poll',\n",
       " 'done',\n",
       " 'summary',\n",
       " 'many',\n",
       " 'year',\n",
       " 'reign',\n",
       " 'caesar',\n",
       " 'young',\n",
       " 'ape',\n",
       " 'go',\n",
       " 'journey',\n",
       " 'lead',\n",
       " 'question',\n",
       " 'everything',\n",
       " 'he',\n",
       " 'taught',\n",
       " 'past',\n",
       " 'make',\n",
       " 'choice',\n",
       " 'define',\n",
       " 'future',\n",
       " 'ape',\n",
       " 'human',\n",
       " 'alike',\n",
       " 'director',\n",
       " 'wes',\n",
       " 'ball',\n",
       " 'writer',\n",
       " 'josh',\n",
       " 'friedman',\n",
       " 'rick',\n",
       " 'jaffa',\n",
       " 'amanda',\n",
       " 'silver',\n",
       " 'cast',\n",
       " 'freya',\n",
       " 'allan',\n",
       " 'mae',\n",
       " 'kevin',\n",
       " 'durand',\n",
       " 'proximus',\n",
       " 'dichen',\n",
       " 'lachman',\n",
       " 'william',\n",
       " 'macy',\n",
       " 'owen',\n",
       " 'teague',\n",
       " 'noa',\n",
       " 'peter',\n",
       " 'macon',\n",
       " 'raka',\n",
       " 'sara',\n",
       " 'wiseman',\n",
       " 'dar',\n",
       " 'rotten',\n",
       " 'tomato',\n",
       " 'metacritic',\n",
       " 'vod',\n",
       " 'theater']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get relevant data\n",
    "df['title'] = df['title'].fillna('')\n",
    "df['body'] = df['body'].fillna('')\n",
    "\n",
    "posts = pd.DataFrame()\n",
    "posts['text'] = df['title'] + ' ' + df['body']\n",
    "\n",
    "# Remove links\n",
    "posts['text_processed'] = posts['text'].map(lambda x: re.sub(r'http\\S+', '', x))\n",
    "\n",
    "# remove special chars and numbers\n",
    "posts['text_processed'] = re.sub(\"[^A-Za-z]+\", \" \", posts['text_processed'])\n",
    "\n",
    "# Convert to lowercase\n",
    "posts['text_processed'] = posts['text_processed'].map(lambda x: x.lower())\n",
    "\n",
    "# Tokenize\n",
    "posts['text_processed'] = posts['text_processed'].map(word_tokenize)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "posts['text_processed'] = posts['text_processed'].map(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# Remove words with less than 3 characters\n",
    "posts['text_processed'] = posts['text_processed'].map(lambda x: [word for word in x if len(word) > 2])\n",
    "\n",
    "# Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "posts['text_processed'] = posts['text_processed'].map(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "# Stemming\n",
    "# stemmer = PorterStemmer()\n",
    "# posts['text_processed'] = posts['text_processed'].map(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "# pd.set_option('display.max_colwidth', 80)\n",
    "posts['text_processed'][12]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['official',\n",
       " 'discussion',\n",
       " 'kingdom_planet',\n",
       " 'ape',\n",
       " 'spoiler',\n",
       " 'poll',\n",
       " 'youve',\n",
       " 'seen',\n",
       " 'film',\n",
       " 'please',\n",
       " 'rate',\n",
       " 'poll',\n",
       " 'havent',\n",
       " 'seen',\n",
       " 'film',\n",
       " 'would',\n",
       " 'like',\n",
       " 'see',\n",
       " 'result',\n",
       " 'poll',\n",
       " 'click',\n",
       " 'ranking',\n",
       " 'click',\n",
       " 'see',\n",
       " 'ranking',\n",
       " '2024',\n",
       " 'film',\n",
       " 'click',\n",
       " 'see',\n",
       " 'ranking',\n",
       " 'every',\n",
       " 'poll',\n",
       " 'done',\n",
       " 'summary',\n",
       " 'many',\n",
       " 'year',\n",
       " 'reign',\n",
       " 'caesar',\n",
       " 'young',\n",
       " 'ape',\n",
       " 'go',\n",
       " 'journey',\n",
       " 'lead',\n",
       " 'question',\n",
       " 'everything',\n",
       " 'he',\n",
       " 'taught',\n",
       " 'past',\n",
       " 'make',\n",
       " 'choice',\n",
       " 'define',\n",
       " 'future',\n",
       " 'ape',\n",
       " 'human',\n",
       " 'alike',\n",
       " 'director',\n",
       " 'wes',\n",
       " 'ball',\n",
       " 'writer',\n",
       " 'josh',\n",
       " 'friedman',\n",
       " 'rick',\n",
       " 'jaffa',\n",
       " 'amanda',\n",
       " 'silver',\n",
       " 'cast',\n",
       " 'freya',\n",
       " 'allan',\n",
       " 'mae',\n",
       " 'kevin',\n",
       " 'durand',\n",
       " 'proximus',\n",
       " 'dichen',\n",
       " 'lachman',\n",
       " 'william',\n",
       " 'macy',\n",
       " 'owen',\n",
       " 'teague',\n",
       " 'noa',\n",
       " 'peter',\n",
       " 'macon',\n",
       " 'raka',\n",
       " 'sara',\n",
       " 'wiseman',\n",
       " 'dar',\n",
       " 'rotten_tomato',\n",
       " 'metacritic',\n",
       " 'vod',\n",
       " 'theater']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Phrase modeling (bigrams and trigrams)\n",
    "bigram = gensim.models.Phrases(posts['text_processed'], min_count=20, threshold=100)\n",
    "# trigram = gensim.models.Phrases(bigram[posts['text_processed']], threshold=100)\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "# trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "# def make_trigrams(texts):\n",
    "#     return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "posts['text_processed'] = make_bigrams(posts['text_processed'])\n",
    "# posts['text_processed'] = make_trigrams(posts['text_processed'])\n",
    "posts['text_processed'][12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dictionary and corpus needed for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary\n",
    "id2word = corpora.Dictionary(posts['text_processed'])\n",
    "# Filter out words\n",
    "id2word.filter_extremes(no_below=10, no_above=0.6)\n",
    "\n",
    "# Create a corpus\n",
    "texts = posts['text_processed']\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list containing various hyperparameters\n",
    "no_of_topics = [10,20,30,40,50,60,70,80,90,100]\n",
    "alpha_list = [\"symmetric\", 0.3]\n",
    "beta_list = [0.3, 0.7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin seach for optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=10, alpha=symmetric, beta=0.3 -> Coherence Score: 0.3769444772404545\n",
      "n=10, alpha=symmetric, beta=0.7 -> Coherence Score: 0.3871068566971324\n",
      "n=10, alpha=0.3, beta=0.3 -> Coherence Score: 0.38268125600567665\n",
      "n=10, alpha=0.3, beta=0.7 -> Coherence Score: 0.3911212129371958\n",
      "n=20, alpha=symmetric, beta=0.3 -> Coherence Score: 0.4170348150388139\n",
      "n=20, alpha=symmetric, beta=0.7 -> Coherence Score: 0.45916982911726223\n",
      "n=20, alpha=0.3, beta=0.3 -> Coherence Score: 0.3851240893945039\n",
      "n=20, alpha=0.3, beta=0.7 -> Coherence Score: 0.42511493007333884\n",
      "n=30, alpha=symmetric, beta=0.3 -> Coherence Score: 0.4570433784003693\n",
      "n=30, alpha=symmetric, beta=0.7 -> Coherence Score: 0.49189028856498335\n",
      "n=30, alpha=0.3, beta=0.3 -> Coherence Score: 0.4066987754249599\n",
      "n=30, alpha=0.3, beta=0.7 -> Coherence Score: 0.43566172101692974\n",
      "n=40, alpha=symmetric, beta=0.3 -> Coherence Score: 0.45990869266585055\n",
      "n=40, alpha=symmetric, beta=0.7 -> Coherence Score: 0.4987158490280993\n",
      "n=40, alpha=0.3, beta=0.3 -> Coherence Score: 0.4224788142787427\n",
      "n=40, alpha=0.3, beta=0.7 -> Coherence Score: 0.46779694880666717\n",
      "n=50, alpha=symmetric, beta=0.3 -> Coherence Score: 0.4727431037470716\n",
      "n=50, alpha=symmetric, beta=0.7 -> Coherence Score: 0.4872878340542174\n",
      "n=50, alpha=0.3, beta=0.3 -> Coherence Score: 0.43353089298173925\n",
      "n=50, alpha=0.3, beta=0.7 -> Coherence Score: 0.49275352450037696\n",
      "n=60, alpha=symmetric, beta=0.3 -> Coherence Score: 0.4919907087467528\n",
      "n=60, alpha=symmetric, beta=0.7 -> Coherence Score: 0.5028045067104105\n",
      "n=60, alpha=0.3, beta=0.3 -> Coherence Score: 0.45183178589242107\n",
      "n=60, alpha=0.3, beta=0.7 -> Coherence Score: 0.5081980115350652\n",
      "n=70, alpha=symmetric, beta=0.3 -> Coherence Score: 0.48629558036426995\n",
      "n=70, alpha=symmetric, beta=0.7 -> Coherence Score: 0.5098622778668414\n",
      "n=70, alpha=0.3, beta=0.3 -> Coherence Score: 0.4626593965339632\n",
      "n=70, alpha=0.3, beta=0.7 -> Coherence Score: 0.5134544474996446\n",
      "n=80, alpha=symmetric, beta=0.3 -> Coherence Score: 0.5047729025663438\n",
      "n=80, alpha=symmetric, beta=0.7 -> Coherence Score: 0.5229594219724489\n",
      "n=80, alpha=0.3, beta=0.3 -> Coherence Score: 0.491350068670066\n",
      "n=80, alpha=0.3, beta=0.7 -> Coherence Score: 0.542528345414319\n",
      "n=90, alpha=symmetric, beta=0.3 -> Coherence Score: 0.5098640362847977\n",
      "n=90, alpha=symmetric, beta=0.7 -> Coherence Score: 0.5201553565252152\n",
      "n=90, alpha=0.3, beta=0.3 -> Coherence Score: 0.49295867515927044\n",
      "n=90, alpha=0.3, beta=0.7 -> Coherence Score: 0.547261942422681\n",
      "n=100, alpha=symmetric, beta=0.3 -> Coherence Score: 0.5054716617015204\n",
      "n=100, alpha=symmetric, beta=0.7 -> Coherence Score: 0.5273767398556415\n",
      "n=100, alpha=0.3, beta=0.3 -> Coherence Score: 0.513507101554437\n",
      "n=100, alpha=0.3, beta=0.7 -> Coherence Score: 0.5422848914889624\n"
     ]
    }
   ],
   "source": [
    "def calculate_coherence_score(n, alpha, beta):\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        num_topics=n,\n",
    "        random_state=100,\n",
    "        update_every=1,\n",
    "        chunksize=100,\n",
    "        passes=3,\n",
    "        alpha=alpha,\n",
    "        per_word_topics=True,\n",
    "        eta=beta,\n",
    "    )\n",
    "\n",
    "    coherence_model_lda = CoherenceModel(\n",
    "        model=lda_model,\n",
    "        texts=posts[\"text_processed\"],\n",
    "        dictionary=id2word,\n",
    "        coherence=\"c_v\",\n",
    "    )\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    return coherence_lda\n",
    "\n",
    "\n",
    "# save param of highest coherence score\n",
    "highest_coherence_score = 0\n",
    "highest_coherence_score_param = (0, 0, 0)\n",
    "\n",
    "# loop through all hyperparameters\n",
    "for n in no_of_topics:\n",
    "    for alpha in alpha_list:\n",
    "        for beta in beta_list:\n",
    "            coherence_score = calculate_coherence_score(n, alpha, beta)\n",
    "            print(\n",
    "                f\"n={n}, alpha={alpha}, beta={beta} -> Coherence Score: {coherence_score}\"\n",
    "            )\n",
    "            if coherence_score > highest_coherence_score:\n",
    "                highest_coherence_score = coherence_score\n",
    "                highest_coherence_score_param = (n, alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.5575884163540132\n"
     ]
    }
   ],
   "source": [
    "# n, alpha, beta = highest_coherence_score_param\n",
    "n, alpha, beta = highest_coherence_score_param\n",
    "lda_model = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=n,\n",
    "    random_state=100,\n",
    "    update_every=1,\n",
    "    chunksize=100,\n",
    "    passes=10,\n",
    "    alpha=alpha,\n",
    "    per_word_topics=True,\n",
    "    eta=beta,\n",
    ")\n",
    "coherence_model_lda = CoherenceModel(\n",
    "    model=lda_model, texts=posts[\"text_processed\"], dictionary=id2word, coherence=\"c_v\"\n",
    ")\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(\"Parameters of highest coherence score:\", highest_coherence_score_param)\n",
    "print(\"\\nCoherence Score: \", coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "# vis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
