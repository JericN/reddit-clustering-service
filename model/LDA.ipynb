{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package installation\n",
    "# %pip install --upgrade matplotlib\n",
    "# %pip install --upgrade numpy\n",
    "# %pip install --upgrade pandas\n",
    "# %pip install --upgrade seaborn\n",
    "# %pip install --upgrade scikit-learn\n",
    "# %pip install --upgrade scipy==1.12\n",
    "# %pip install --upgrade nltk\n",
    "# %pip install --upgrade wordcloud\n",
    "# %pip install --upgrade gensim\n",
    "# %pip install --upgrade pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "# Scientific computing\n",
    "import scipy\n",
    "# Regular expression operations\n",
    "import re\n",
    "# Common string operations\n",
    "import string \n",
    "\n",
    "# Interpret the results of the LDA model\n",
    "import pyLDAvis\n",
    "# Interactive data visualization\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# Data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Unsupervised topic modeling, document indexing.\n",
    "import gensim\n",
    "# Mapping of the words to integers\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Natural language processing\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('wordnet') \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# formatting\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# set pd column width\n",
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv('../data/data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relevant data\n",
    "df['title'] = df['title'].fillna('')\n",
    "df['body'] = df['body'].fillna('')\n",
    "\n",
    "posts = pd.DataFrame()\n",
    "posts['text'] = df['title'] + ' ' + df['body']\n",
    "\n",
    "# Remove punctuation\n",
    "posts['text_processed'] = posts['text'].map(lambda x: re.sub('[,.!?]', '', x))\n",
    "\n",
    "# Convert to lowercase\n",
    "posts['text_processed'] = posts['text_processed'].map(lambda x: x.lower())\n",
    "\n",
    "#removing digits\n",
    "posts['text_processed'] = posts['text_processed'].map(lambda x: re.sub('d+', '', x))\n",
    "\n",
    "# Tokenize\n",
    "posts['text_processed'] = posts['text_processed'].map(word_tokenize)\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "posts['text_processed'] = posts['text_processed'].map(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# Remove words with less than 3 characters\n",
    "posts['text_processed'] = posts['text_processed'].map(lambda x: [word for word in x if len(word) > 2])\n",
    "\n",
    "# Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "posts['text_processed'] = posts['text_processed'].map(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "# Stemming\n",
    "# stemmer = PorterStemmer()\n",
    "# posts['text_processed'] = posts['text_processed'].map(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "\n",
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrase modeling (bigrams and trigrams)\n",
    "bigram = gensim.models.Phrases(posts['text_processed'], min_count=5, threshold=100)\n",
    "# trigram = gensim.models.Phrases(bigram[posts['text_processed']], threshold=100)\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "# trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "# def make_trigrams(texts):\n",
    "#     return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "posts['text_processed'] = make_bigrams(posts['text_processed'])\n",
    "# posts['text_processed'] = make_trigrams(posts['text_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary\n",
    "id2word = corpora.Dictionary(posts['text_processed'])\n",
    "# Filter out words\n",
    "id2word.filter_extremes(no_below=20, no_above=0.8)\n",
    "\n",
    "# Create a corpus\n",
    "texts = posts['text_processed']\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coherence_score(n, alpha, beta):\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        num_topics=n,\n",
    "        random_state=100,\n",
    "        update_every=1,\n",
    "        chunksize=100,\n",
    "        passes=10,\n",
    "        alpha=alpha,\n",
    "        per_word_topics=True,\n",
    "        eta=beta,\n",
    "    )\n",
    "\n",
    "    coherence_model_lda = CoherenceModel(\n",
    "        model=lda_model,\n",
    "        texts=posts[\"text_processed\"],\n",
    "        dictionary=id2word,\n",
    "        coherence=\"c_v\",\n",
    "    )\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    return coherence_lda\n",
    "\n",
    "\n",
    "# list containing various hyperparameters\n",
    "no_of_topics = [10, 40, 60, 80, 100, 120, 140, 160, 180, 200]\n",
    "alpha_list = [\"symmetric\", 0.3]\n",
    "beta_list = [0.3, 0.7]\n",
    "\n",
    "\n",
    "# save param of highest coherence score\n",
    "highest_coherence_score = 0\n",
    "highest_coherence_score_param = (0, 0, 0)\n",
    "\n",
    "# loop through all hyperparameters\n",
    "for n in no_of_topics:\n",
    "    for alpha in alpha_list:\n",
    "        for beta in beta_list:\n",
    "            coherence_score = calculate_coherence_score(n, alpha, beta)\n",
    "            print(\n",
    "                f\"n={n}, alpha={alpha}, beta={beta} -> Coherence Score: {coherence_score}\"\n",
    "            )\n",
    "            if coherence_score > highest_coherence_score:\n",
    "                highest_coherence_score = coherence_score\n",
    "                highest_coherence_score_param = (n, alpha, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, alpha, beta = highest_coherence_score_param\n",
    "lda_model = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=n,\n",
    "    random_state=100,\n",
    "    update_every=1,\n",
    "    chunksize=100,\n",
    "    passes=10,\n",
    "    alpha=alpha,\n",
    "    per_word_topics=True,\n",
    "    eta=beta,\n",
    ")\n",
    "coherence_model_lda = CoherenceModel(\n",
    "    model=lda_model, texts=posts[\"text_processed\"], dictionary=id2word, coherence=\"c_v\"\n",
    ")\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(\"\\nCoherence Score: \", coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
