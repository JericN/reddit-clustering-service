{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA) \n",
    "LDA is a tool for finding topics in a collection of documents. It assumes that each document is a mix of topics, and each topic is a mix of words. The goal is to uncover these topics from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package installation\n",
    "# %pip install --upgrade matplotlib\n",
    "# %pip install --upgrade numpy\n",
    "# %pip install --upgrade pandas\n",
    "# %pip install --upgrade seaborn\n",
    "# %pip install --upgrade scikit-learn\n",
    "# %pip install --upgrade scipy==1.12\n",
    "# %pip install --upgrade nltk\n",
    "# %pip install --upgrade wordcloud\n",
    "# %pip install --upgrade gensim\n",
    "# %pip install --upgrade pyLDAvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\skybl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\skybl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\skybl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "# Scientific computing\n",
    "import scipy\n",
    "# Regular expression operations\n",
    "import re\n",
    "# Common string operations\n",
    "import string \n",
    "\n",
    "# Interpret the results of the LDA model\n",
    "import pyLDAvis\n",
    "# Interactive data visualization\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# Data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Unsupervised topic modeling, document indexing.\n",
    "import gensim\n",
    "# Mapping of the words to integers\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import Phrases, phrases\n",
    "\n",
    "# Natural language processing\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('wordnet') \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# formatting\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# set pd column width\n",
    "pd.set_option('display.max_colwidth', 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(dir: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(dir)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    data = pd.DataFrame()\n",
    "\n",
    "    # Get relevant data\n",
    "    data[\"title\"] = df[\"title\"].fillna(\"\")\n",
    "    data[\"body\"] = df[\"body\"].fillna(\"\")\n",
    "\n",
    "    # Combine title and body\n",
    "    data[\"text\"] = data[\"title\"] + \" \" + data[\"body\"]\n",
    "\n",
    "    # Remove links\n",
    "    data[\"processed\"] = data[\"text\"].map(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
    "\n",
    "    # Remove punctuation\n",
    "    data[\"processed\"] = data[\"processed\"].map(lambda x: x.translate(str.maketrans(\"\", \"\", string.punctuation)))\n",
    "\n",
    "    # Convert to lowercase\n",
    "    data[\"processed\"] = data[\"processed\"].map(lambda x: x.lower())\n",
    "\n",
    "    # Tokenize\n",
    "    data[\"processed\"] = data[\"processed\"].map(word_tokenize)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    data[\"processed\"] = data[\"processed\"].map(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "    # Remove words with less than 3 characters\n",
    "    data[\"processed\"] = data[\"processed\"].map(lambda x: [word for word in x if len(word) >= 3])\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    data[\"processed\"] = data[\"processed\"].map(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "    # Stemming\n",
    "    # stemmer = PorterStemmer()\n",
    "    # data['processed'] = data['processed'].map(lambda x: [stemmer.stem(word) for word in x])\n",
    "\n",
    "    # Remove rows with less than 5 words\n",
    "    data = data[data[\"processed\"].map(len) >= 10]\n",
    "\n",
    "    # Remove unnecessary columns\n",
    "    data.drop([\"title\", \"body\"], axis=1, inplace=True)\n",
    "\n",
    "    # reset index\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngrams(texts, ngram_model):\n",
    "    return [ngram_model[doc] for doc in texts]\n",
    "\n",
    "\n",
    "def generate_ngrams(data, n=2, min_count=5, threshold=50):\n",
    "    dataset = data.copy()\n",
    "\n",
    "    for i in range(2, n+1):\n",
    "        ngram = Phrases(dataset, min_count=min_count, threshold=threshold)\n",
    "        ngram_model = phrases.Phraser(ngram)\n",
    "        dataset = make_ngrams(dataset, ngram_model)\n",
    "\n",
    "    return dataset\n",
    "    \n",
    "\n",
    "def print_ngrams(data, n=2):\n",
    "    ngrams_set = set()\n",
    "    for row in data:\n",
    "        for word in row:\n",
    "            if word.count(\"_\") >= n - 1:\n",
    "                ngrams_set.add(word)\n",
    "\n",
    "    print(\"length of ngrams set: \", len(ngrams_set))\n",
    "    print(ngrams_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dictionary and corpus needed for topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(data: pd.DataFrame) -> tuple:\n",
    "    dataset = data.copy()\n",
    "\n",
    "    # Create a dictionary\n",
    "    id2word = corpora.Dictionary(dataset)\n",
    "    # Filter out words\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.6)\n",
    "\n",
    "    # Create a corpus\n",
    "    corpus = [id2word.doc2bow(text) for text in dataset]\n",
    "\n",
    "    return id2word, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to build the LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda_model(id2word, corpus, alpha=\"symmetric\", beta=\"auto\", topics=10):\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=id2word,\n",
    "        num_topics=topics,\n",
    "        random_state=100,\n",
    "        update_every=1,\n",
    "        chunksize=100,\n",
    "        passes=5,\n",
    "        alpha=alpha,\n",
    "        per_word_topics=True,\n",
    "        eta=beta,\n",
    "    )\n",
    "\n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coherence_score(data, id2word, corpus, alpha, beta, topics):\n",
    "    lda_model = train_lda_model(id2word, corpus, alpha, beta, topics)\n",
    "\n",
    "    coherence_model_lda = CoherenceModel(\n",
    "        model=lda_model,\n",
    "        texts=data,\n",
    "        dictionary=id2word,\n",
    "        coherence=\"c_v\",\n",
    "    )\n",
    "    coherence_score = coherence_model_lda.get_coherence()\n",
    "    return coherence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to iterate through hyperparameters\n",
    "def find_best_model(data, id2word, corpus, alpha_list, beta_list, topics_list):\n",
    "    # save param of highest coherence score\n",
    "    best_params = {\"score\": 0, \"alpha\": 0, \"beta\": 0, \"topics\": 0}\n",
    "\n",
    "    # loop through all hyperparameters\n",
    "    for topics in topics_list:\n",
    "        for alpha in alpha_list:\n",
    "            for beta in beta_list:\n",
    "                # calculate coherence score\n",
    "                coherence_score = calculate_coherence_score(data, id2word, corpus, alpha, beta, topics)\n",
    "                print(f\"topics={topics}, alpha={alpha}, beta={beta} -> Coherence Score: {coherence_score}\")\n",
    "\n",
    "                # update best params if new score is higher\n",
    "                if coherence_score > best_params[\"score\"]:\n",
    "                    best_params[\"score\"] = coherence_score\n",
    "                    best_params[\"alpha\"] = alpha\n",
    "                    best_params[\"beta\"] = beta\n",
    "                    best_params[\"topics\"] = topics\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START LDA MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (3756, 2)\n",
      "Number of unique words: 2712\n",
      "Number of documents: 3756\n",
      "\n",
      "\n",
      "Starting hyperparameter tuning...\n",
      "topics=10, alpha=symmetric, beta=0.1 -> Coherence Score: 0.3837150134849368\n",
      "topics=10, alpha=symmetric, beta=0.5 -> Coherence Score: 0.42537215998467365\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting hyperparameter tuning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# find best model\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m best_params \u001b[38;5;241m=\u001b[39m \u001b[43mfind_best_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprocessed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid2word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_of_topics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_params)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# train lda model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m, in \u001b[0;36mfind_best_model\u001b[1;34m(data, id2word, corpus, alpha_list, beta_list, topics_list)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m alpha_list:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m beta \u001b[38;5;129;01min\u001b[39;00m beta_list:\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;66;03m# calculate coherence score\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m         coherence_score \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_coherence_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid2word\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopics=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopics\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, alpha=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, beta=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbeta\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> Coherence Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoherence_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;66;03m# update best params if new score is higher\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 10\u001b[0m, in \u001b[0;36mcalculate_coherence_score\u001b[1;34m(data, id2word, corpus, alpha, beta, topics)\u001b[0m\n\u001b[0;32m      2\u001b[0m lda_model \u001b[38;5;241m=\u001b[39m train_lda_model(id2word, corpus, alpha, beta, topics)\n\u001b[0;32m      4\u001b[0m coherence_model_lda \u001b[38;5;241m=\u001b[39m CoherenceModel(\n\u001b[0;32m      5\u001b[0m     model\u001b[38;5;241m=\u001b[39mlda_model,\n\u001b[0;32m      6\u001b[0m     texts\u001b[38;5;241m=\u001b[39mdata,\n\u001b[0;32m      7\u001b[0m     dictionary\u001b[38;5;241m=\u001b[39mid2word,\n\u001b[0;32m      8\u001b[0m     coherence\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc_v\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m )\n\u001b[1;32m---> 10\u001b[0m coherence_score \u001b[38;5;241m=\u001b[39m \u001b[43mcoherence_model_lda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_coherence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m coherence_score\n",
      "File \u001b[1;32mc:\\Users\\skybl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\models\\coherencemodel.py:614\u001b[0m, in \u001b[0;36mCoherenceModel.get_coherence\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_coherence\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    606\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get coherence value based on pipeline parameters.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \n\u001b[0;32m    608\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    612\u001b[0m \n\u001b[0;32m    613\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 614\u001b[0m     confirmed_measures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_coherence_per_topic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    615\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_measures(confirmed_measures)\n",
      "File \u001b[1;32mc:\\Users\\skybl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\models\\coherencemodel.py:574\u001b[0m, in \u001b[0;36mCoherenceModel.get_coherence_per_topic\u001b[1;34m(self, segmented_topics, with_std, with_support)\u001b[0m\n\u001b[0;32m    572\u001b[0m     segmented_topics \u001b[38;5;241m=\u001b[39m measure\u001b[38;5;241m.\u001b[39mseg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopics)\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 574\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimate_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegmented_topics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(with_std\u001b[38;5;241m=\u001b[39mwith_std, with_support\u001b[38;5;241m=\u001b[39mwith_support)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoherence \u001b[38;5;129;01min\u001b[39;00m BOOLEAN_DOCUMENT_BASED \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoherence \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_w2v\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\skybl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\models\\coherencemodel.py:546\u001b[0m, in \u001b[0;36mCoherenceModel.estimate_probabilities\u001b[1;34m(self, segmented_topics)\u001b[0m\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoherence \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_w2v\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    544\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeyed_vectors\n\u001b[1;32m--> 546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeasure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accumulator\n",
      "File \u001b[1;32mc:\\Users\\skybl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\topic_coherence\\probability_estimation.py:156\u001b[0m, in \u001b[0;36mp_boolean_sliding_window\u001b[1;34m(texts, segmented_topics, dictionary, window_size, processes)\u001b[0m\n\u001b[0;32m    154\u001b[0m     accumulator \u001b[38;5;241m=\u001b[39m ParallelWordOccurrenceAccumulator(processes, top_ids, dictionary)\n\u001b[0;32m    155\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to estimate probabilities from sliding windows\u001b[39m\u001b[38;5;124m\"\u001b[39m, accumulator)\n\u001b[1;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maccumulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccumulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\skybl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\topic_coherence\\text_analysis.py:437\u001b[0m, in \u001b[0;36mParallelWordOccurrenceAccumulator.accumulate\u001b[1;34m(self, texts, window_size)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccumulate\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts, window_size):\n\u001b[1;32m--> 437\u001b[0m     workers, input_q, output_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    439\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueue_all_texts(input_q, texts, window_size)\n",
      "File \u001b[1;32mc:\\Users\\skybl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\topic_coherence\\text_analysis.py:471\u001b[0m, in \u001b[0;36mParallelWordOccurrenceAccumulator.start_workers\u001b[1;34m(self, window_size)\u001b[0m\n\u001b[0;32m    469\u001b[0m     accumulator \u001b[38;5;241m=\u001b[39m PatchedWordOccurrenceAccumulator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelevant_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdictionary)\n\u001b[0;32m    470\u001b[0m     worker \u001b[38;5;241m=\u001b[39m AccumulatingWorker(input_q, output_q, accumulator, window_size)\n\u001b[1;32m--> 471\u001b[0m     \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m     workers\u001b[38;5;241m.\u001b[39mappend(worker)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m workers, input_q, output_q\n",
      "File \u001b[1;32mc:\\Users\\skybl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\skybl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\skybl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\context.py:337\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\skybl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\popen_spawn_win32.py:95\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 95\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\skybl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import\n",
    "df = import_data(\"../data/data.csv\")\n",
    "\n",
    "# preprocess\n",
    "posts = preprocess_data(df)\n",
    "\n",
    "# generate n-grams\n",
    "posts[\"processed\"] = generate_ngrams(posts[\"processed\"], n=3, min_count=5, threshold=100)\n",
    "print(\"Data Shape:\", posts.shape)\n",
    "\n",
    "# create corpus\n",
    "id2word, corpus = create_corpus(posts[\"processed\"])\n",
    "print(\"Number of unique words:\", len(id2word))\n",
    "print(\"Number of documents:\", len(corpus))\n",
    "\n",
    "# hyperparameters\n",
    "no_of_topics = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "alpha_list = [\"symmetric\", 0.1, 0.5, 0.9]\n",
    "beta_list = [0.1, 0.5, 0.9]\n",
    "print(\"\\n\\nStarting hyperparameter tuning...\")\n",
    "\n",
    "# find best model\n",
    "best_params = find_best_model(posts[\"processed\"], id2word, corpus, alpha_list, beta_list, no_of_topics)\n",
    "print(best_params)\n",
    "\n",
    "\n",
    "# train lda model\n",
    "lda_model = train_lda_model(\n",
    "    id2word,\n",
    "    corpus,\n",
    "    best_params[\"alpha\"],\n",
    "    best_params[\"beta\"],\n",
    "    best_params[\"topics\"],\n",
    ")\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
